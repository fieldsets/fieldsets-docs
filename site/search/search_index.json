{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fieldsets Data Pipeline Documentation Table Of Contents Install Guide Environment Variables Tools Developing With fieldsets-pipeline Running The Pipeline Locally Future Directions List of Pipeline Projects","title":"Home"},{"location":"#fieldsets-data-pipeline-documentation","text":"","title":"Fieldsets Data Pipeline Documentation"},{"location":"#table-of-contents","text":"Install Guide Environment Variables Tools Developing With fieldsets-pipeline Running The Pipeline Locally Future Directions List of Pipeline Projects","title":"Table Of Contents"},{"location":"data-field-types/","text":"fieldset string number decimal object list array vector bool date ts search uuid function custom","title":"Field Types"},{"location":"data-field-types/#fieldset","text":"","title":"fieldset"},{"location":"data-field-types/#string","text":"","title":"string"},{"location":"data-field-types/#number","text":"","title":"number"},{"location":"data-field-types/#decimal","text":"","title":"decimal"},{"location":"data-field-types/#object","text":"","title":"object"},{"location":"data-field-types/#list","text":"","title":"list"},{"location":"data-field-types/#array","text":"","title":"array"},{"location":"data-field-types/#vector","text":"","title":"vector"},{"location":"data-field-types/#bool","text":"","title":"bool"},{"location":"data-field-types/#date","text":"","title":"date"},{"location":"data-field-types/#ts","text":"","title":"ts"},{"location":"data-field-types/#search","text":"","title":"search"},{"location":"data-field-types/#uuid","text":"","title":"uuid"},{"location":"data-field-types/#function","text":"","title":"function"},{"location":"data-field-types/#custom","text":"","title":"custom"},{"location":"data-store-types/","text":"Data Stores The Fieldsets Framework revolves around the concept of data stores. Data Stores can be thought of as Standardized Table Structures. Each store is structured to efficiently utilize the technology on which the data is stored. Data Store Types Currently the frame work has 9 predefined data store types. Filter Filters operate as traditional relational data tables. A field stored as a filter will always contain the primary key of it's parent fieldset. The fields stored are used to filter data and rely on foreign keys to other data stores to optimize access to corresponding data stores. Currently using Postgres data tables but can be migrated to other RDBMS if needed. Along with any filter fields defined for a fieldset, all Filter Stores will minimally contain the columns: id token parent_id parent_token created updated BIGINT STRING BIGINT STRING TIMESTAMP TIMESTAMP Record Records operate as time series data stores. Currently using Clickhouse MergeTree Table Engines partitioned on date. Along with any record fields defined for a fieldset, all Record Stores will minimally contain the columns: id parent_id created BIGINT BIGINT TIMESTAMP Also any insert into a record store will update the updated column of matching ids in any related foreign key filter stores. Lookup Lookup tables are 1 to 1 relationship or lookup tables. Currently stored using Clickhouse Merge Dictionaries as our lookup tables. Document Documents are semi-structured data stores that can be indexed. Currently stored using Postgresql JSONB data types. Document Stores will be added as a JSONB column onto any existing filter tables. Sequence Sequences are stored using positional data. Currently using Clickhouse MergeTree Table Engines partitioned on data positions. Stream Streams are build for write performance with minimal indexing. Logging and message queues are similar. Currently using Postgresql Unlogged tables with TEXT data types. TODO: Integrate Kafka and Clickhouse Log storage. View Views are an assembly of previously defined data fields combined into single table like structure. These views are typically stored in memory. PostgreSQL materialized views are difficult to update when using larger data sets and joins, so Clickhouse Materialized views are used to deliver this useful store type. Enum Enums are a set of named constants. Instead of stored as a table structure, these are created as a set of usable values for a given field via the PostgreSQL and Clickhouse ENUM data types. File Files are structured or unstructured files stored on the filesystem. Uses Clickhouse file table engine. Program Programs transform a field using functions or applications and generate the field from other data stores. Custom Custom data stores can be defined.","title":"Store Types"},{"location":"data-store-types/#data-stores","text":"The Fieldsets Framework revolves around the concept of data stores. Data Stores can be thought of as Standardized Table Structures. Each store is structured to efficiently utilize the technology on which the data is stored.","title":"Data Stores"},{"location":"data-store-types/#data-store-types","text":"Currently the frame work has 9 predefined data store types.","title":"Data Store Types"},{"location":"data-store-types/#filter","text":"Filters operate as traditional relational data tables. A field stored as a filter will always contain the primary key of it's parent fieldset. The fields stored are used to filter data and rely on foreign keys to other data stores to optimize access to corresponding data stores. Currently using Postgres data tables but can be migrated to other RDBMS if needed. Along with any filter fields defined for a fieldset, all Filter Stores will minimally contain the columns: id token parent_id parent_token created updated BIGINT STRING BIGINT STRING TIMESTAMP TIMESTAMP","title":"Filter"},{"location":"data-store-types/#record","text":"Records operate as time series data stores. Currently using Clickhouse MergeTree Table Engines partitioned on date. Along with any record fields defined for a fieldset, all Record Stores will minimally contain the columns: id parent_id created BIGINT BIGINT TIMESTAMP Also any insert into a record store will update the updated column of matching ids in any related foreign key filter stores.","title":"Record"},{"location":"data-store-types/#lookup","text":"Lookup tables are 1 to 1 relationship or lookup tables. Currently stored using Clickhouse Merge Dictionaries as our lookup tables.","title":"Lookup"},{"location":"data-store-types/#document","text":"Documents are semi-structured data stores that can be indexed. Currently stored using Postgresql JSONB data types. Document Stores will be added as a JSONB column onto any existing filter tables.","title":"Document"},{"location":"data-store-types/#sequence","text":"Sequences are stored using positional data. Currently using Clickhouse MergeTree Table Engines partitioned on data positions.","title":"Sequence"},{"location":"data-store-types/#stream","text":"Streams are build for write performance with minimal indexing. Logging and message queues are similar. Currently using Postgresql Unlogged tables with TEXT data types. TODO: Integrate Kafka and Clickhouse Log storage.","title":"Stream"},{"location":"data-store-types/#view","text":"Views are an assembly of previously defined data fields combined into single table like structure. These views are typically stored in memory. PostgreSQL materialized views are difficult to update when using larger data sets and joins, so Clickhouse Materialized views are used to deliver this useful store type.","title":"View"},{"location":"data-store-types/#enum","text":"Enums are a set of named constants. Instead of stored as a table structure, these are created as a set of usable values for a given field via the PostgreSQL and Clickhouse ENUM data types.","title":"Enum"},{"location":"data-store-types/#file","text":"Files are structured or unstructured files stored on the filesystem. Uses Clickhouse file table engine.","title":"File"},{"location":"data-store-types/#program","text":"Programs transform a field using functions or applications and generate the field from other data stores.","title":"Program"},{"location":"data-store-types/#custom","text":"Custom data stores can be defined.","title":"Custom"},{"location":"develop/","text":"Developing with Fieldsets Pipeline's docker containers fieldsets-local docker-starter list of environment variables current integrations","title":"Developing with Fieldsets Pipeline's docker containers"},{"location":"develop/#developing-with-fieldsets-pipelines-docker-containers","text":"fieldsets-local docker-starter list of environment variables current integrations","title":"Developing with Fieldsets Pipeline's docker containers"},{"location":"env-vars/","text":"Fieldsets Framework Environment Variables The Fieldsets Repository is an wrapper of other docker containers. The environment variables it required to run a pipeline derived container will vary on the projects integrated to each pipeline project. The table below lists all the variables utilized within Fieldsets's data pipeline and the projects they are intergrated with. For more details, you can check the environments section of each individual project's README or have a look at the project configuration documentataion found in config/docker . NOTES: - All PATH variables expect a trailing / - Variable tables organized by project usage with largest multi project variables listed first Universal Variables Variable Default Value Valid Values Projects Description VERSION latest any string all Fieldsets pipeline version ENVIRONMENT dev any string all Server environment COMPOSE_PATH_SEPARATOR : : (mac/linux) or ; (windows) all native docker compose variable. Do not change. COMPOSE_FILE ./docker-compose.yml:./config/postgres/docker-compose.yml A : separated string of relative docker compose file paths. all Make sure the top level compose file is first followed by any other partial of full compose files. LOCAL_UID 1000 any integer. Typically 1000 (linux) or 502 (mac) all User id of host account. Use the command id -u to retrieve your user id. This allows you to write to containers w/o permission errors. LOCAL_GID 1000 any integer. Typically 1000 (linux) or 20 (mac) all Group id of host account. Use the command id -g to retrieve the group id. TIMEZONE America/New_York Any valid timezone string all Timezone of the container host server FIELDSETS_NETWORK_SUBNET 172.28.0.0/24 any valid ipv4 Class A range address all The address range subnet which containers can communicate under FIELDSETS_SRC_PATH ./src/ any relative path string fieldsets-local Mounts this path to /fieldsets within container Multi-container Variables Variable Default Value Valid Values Projects Description POSTGRES_VERSION 15 any valid major version number fieldsets-local docker-postgres PostgreSQL Major Release Version. POSTGRES_USER postgres any string fieldsets-local docker-postgres PostgreSQL DB username POSTGRES_PASSWORD fieldsets any string fieldsets-local docker-postgres PostgreSQL DB user password POSTGRES_HOST 172.28.0.7 any valid ipv4 address or DNS host string fieldsets-local docker-postgres Address of PostgreSQL host. May be external or containerized address. POSTGRES_PORT 5432 any integer fieldsets-local docker-postgres PostgreSQL port for connection POSTGRES_DB fieldsets any string fieldsets-local docker-postgres PostgreSQL database name ENABLE_TERMINAL false true or false fieldsets-local Sets variable withing docker compose file tty: ${ENABLE_TERMINAL} . Allows for pseudo tty. Postgres Container Variables Variable Default Value Valid Values Projects Description POSTGRES_CONFIG_PATH ./config/postgres/ any valid path docker-postgres Location of Dockerfile. ClickHouse Variables Variable Default Value Valid Values Projects Description ENABLE_STORE false Disabled by default. fieldsets-local Enable to use column store data tables CLICKHOUSE_VERSION 24 any valid major version number none Clickhouse major release version CLICKHOUSE_CONFIG_PATH ./config/clickhouse/ any valid path none CLICKHOUSE_USER default any string none ClickHouse DB username CLICKHOUSE_PASSWORD fieldsets any string none ClickHouse DB user password CLICKHOUSE_HOST 172.28.0.5 any valid ipv4 address or DNS host string none Address of ClickHouse host. May be external or containerized address. CLICKHOUSE_PORT 8123 any integer none ClickHouse port for connection CLICKHOUSE_DB fieldsets any string none ClickHouse DB name Fieldsets Local Variables Variable Default Value Valid Values Projects Description FIELDSETS_SRC_PATH ./src/ any relative path string fieldsets-local Mounts this path to /fieldsets within container FIELDSETS_LOCAL_CONFIG_PATH ./config/local/ any relative path string fieldsets-local Location of Dockerfile. FIELDSETS_LOCAL_HOST 172.28.0.6 any valid ipv4 address fieldsets-local Allows external containers to access this container vi local ip address. SSH_PORT 22 any integer fieldsets-local Port number of Fieldsets jump server SSH_USER NULL any string fieldsets-local Username for Fieldsets jump server login SSH_KEY_PATH ~/.ssh/ any valid file path fieldsets-local RSA key file path","title":"Enviornment Variables"},{"location":"env-vars/#fieldsets-framework-environment-variables","text":"The Fieldsets Repository is an wrapper of other docker containers. The environment variables it required to run a pipeline derived container will vary on the projects integrated to each pipeline project. The table below lists all the variables utilized within Fieldsets's data pipeline and the projects they are intergrated with. For more details, you can check the environments section of each individual project's README or have a look at the project configuration documentataion found in config/docker . NOTES: - All PATH variables expect a trailing / - Variable tables organized by project usage with largest multi project variables listed first","title":"Fieldsets Framework Environment Variables"},{"location":"env-vars/#universal-variables","text":"Variable Default Value Valid Values Projects Description VERSION latest any string all Fieldsets pipeline version ENVIRONMENT dev any string all Server environment COMPOSE_PATH_SEPARATOR : : (mac/linux) or ; (windows) all native docker compose variable. Do not change. COMPOSE_FILE ./docker-compose.yml:./config/postgres/docker-compose.yml A : separated string of relative docker compose file paths. all Make sure the top level compose file is first followed by any other partial of full compose files. LOCAL_UID 1000 any integer. Typically 1000 (linux) or 502 (mac) all User id of host account. Use the command id -u to retrieve your user id. This allows you to write to containers w/o permission errors. LOCAL_GID 1000 any integer. Typically 1000 (linux) or 20 (mac) all Group id of host account. Use the command id -g to retrieve the group id. TIMEZONE America/New_York Any valid timezone string all Timezone of the container host server FIELDSETS_NETWORK_SUBNET 172.28.0.0/24 any valid ipv4 Class A range address all The address range subnet which containers can communicate under FIELDSETS_SRC_PATH ./src/ any relative path string fieldsets-local Mounts this path to /fieldsets within container","title":"Universal Variables"},{"location":"env-vars/#multi-container-variables","text":"Variable Default Value Valid Values Projects Description POSTGRES_VERSION 15 any valid major version number fieldsets-local docker-postgres PostgreSQL Major Release Version. POSTGRES_USER postgres any string fieldsets-local docker-postgres PostgreSQL DB username POSTGRES_PASSWORD fieldsets any string fieldsets-local docker-postgres PostgreSQL DB user password POSTGRES_HOST 172.28.0.7 any valid ipv4 address or DNS host string fieldsets-local docker-postgres Address of PostgreSQL host. May be external or containerized address. POSTGRES_PORT 5432 any integer fieldsets-local docker-postgres PostgreSQL port for connection POSTGRES_DB fieldsets any string fieldsets-local docker-postgres PostgreSQL database name ENABLE_TERMINAL false true or false fieldsets-local Sets variable withing docker compose file tty: ${ENABLE_TERMINAL} . Allows for pseudo tty.","title":"Multi-container Variables"},{"location":"env-vars/#postgres-container-variables","text":"Variable Default Value Valid Values Projects Description POSTGRES_CONFIG_PATH ./config/postgres/ any valid path docker-postgres Location of Dockerfile.","title":"Postgres Container Variables"},{"location":"env-vars/#clickhouse-variables","text":"Variable Default Value Valid Values Projects Description ENABLE_STORE false Disabled by default. fieldsets-local Enable to use column store data tables CLICKHOUSE_VERSION 24 any valid major version number none Clickhouse major release version CLICKHOUSE_CONFIG_PATH ./config/clickhouse/ any valid path none CLICKHOUSE_USER default any string none ClickHouse DB username CLICKHOUSE_PASSWORD fieldsets any string none ClickHouse DB user password CLICKHOUSE_HOST 172.28.0.5 any valid ipv4 address or DNS host string none Address of ClickHouse host. May be external or containerized address. CLICKHOUSE_PORT 8123 any integer none ClickHouse port for connection CLICKHOUSE_DB fieldsets any string none ClickHouse DB name","title":"ClickHouse Variables"},{"location":"env-vars/#fieldsets-local-variables","text":"Variable Default Value Valid Values Projects Description FIELDSETS_SRC_PATH ./src/ any relative path string fieldsets-local Mounts this path to /fieldsets within container FIELDSETS_LOCAL_CONFIG_PATH ./config/local/ any relative path string fieldsets-local Location of Dockerfile. FIELDSETS_LOCAL_HOST 172.28.0.6 any valid ipv4 address fieldsets-local Allows external containers to access this container vi local ip address. SSH_PORT 22 any integer fieldsets-local Port number of Fieldsets jump server SSH_USER NULL any string fieldsets-local Username for Fieldsets jump server login SSH_KEY_PATH ~/.ssh/ any valid file path fieldsets-local RSA key file path","title":"Fieldsets Local Variables"},{"location":"install/","text":"Fieldsets Pipeline Installation Guide This section covers details on how to get started with the fieldsets pipeline. TL;DR git clone --recurse-submodules git@github.com:Fieldsets/fieldsets-pipeline.git cd fieldsets-pipeline git submodule foreach git pull origin main cp ./env.example ./.env (make appropriate corrections to file. additional example envs found in [./config/docker](./config/docker/) docker-compose up -d --build System Requirements. Before we begin, you must have the following installed locally on your machine. Follow the links below for directions on how to get setup on your current environment. docker compose git Mac OS Using the terminal execute the following steps - Install Xcode - xcode-select --install - Install homebrew - /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\" - Disable homebrew analytics - brew analytics off - Install git - brew install git - Install docker-compose - brew install docker-compose - Run caffeinate to prevent sleep on first run - caffeinate Linux (Debian based) Install git sudo apt install docker Install git sudo apt install git Windows Install (wsl v2) Install wsl v2 Install Docker Desktop Set DNS using ./config/docker-daemon.json Install Steps Once you have install git and docker-compose, the first step is to clone the fieldsets-pipeline repository and all of it's submodules into your local environment. This pipeline has 2 submodules minimally required to run. The postgres database container and a localized scripting container that helps run whatever scripts are needed by the pipeline project. By default, this pipeline will run using all variables from all integrations that are active in production even if the containers are not running. Before getting started, it may be best to look in the docker config sub-directory and utilize the dotenv, gitmodule & compose override files found there in place of the top level files used to run the pipeline in full. git clone --recurse-submodules https://github.com/fieldsets/fieldsets-pipeline.git Next we need head to our repository directory and make sure we have the latest versions of our submodules. You can update them in with the following commands: cd fieldsets-pipeline git submodule foreach git pull origin main While the enviroment will boot with default options, you should make a copy of the example env file and change any configuration parameters (like postgres version or the path of your private key). This is unnecessary as the project will run with the default values set in the docker-compose.yml file, but the project will have little functionality out side of creating a local postgres environment for you. If you are looking for a fieldsets database instance to run on EC2, you may want to look at Fieldsets's docker postgres repository. The example env contains all the variables used to configure our full pipeline, so every project (including yours!) that is integrated into the Fieldsets Automated Pipeline, has it's environment variables added to the env.example file . Depending on what you are looking to do, it is likely that many of these variables will be empty for your application or not defined. You can omit or change these variables to help with current deployments. Defaults will be filled in using values defined in the docker-compose file for any omitted values. Subsets of fieldsets-pipeline project dotenv files can be found in config/docker/ . A full explantion of environment variables can also be found here . NOTE: Other versions of PostgreSQL have not been tested outside of v12 & v13. The philosophy behind upgrades is to do it after it has established thorough adoption by the community. cp ./env.example ./.env Now lets get the environment running! For our remote data stores to work, we will need credentials for our production and clone servers. Once docker-compose is installed and you have cloned this repository as instructed above, you can get the entire environment up and running with the following command: docker compose up -d That's it! You should be up and running. The first docker build may take a while as it imports data remotely and can vary depending on network speed and your local environment hardware. If you'd like to view what is happening on the install you can track the output logs of the container that does all the heavy lifting with the following command: docker compose logs -f fieldsets-local or simply check everything docker compose logs -f If you see a log entry of a container exited with code 0 that means everything has run succssfully. Anyother exit code besides 0 means something went wrong somewhere along the line. You you have set ENABLE_TERMINAL=true you won't see an exit with code 0 message when a container completes. You should simply see the final message output by the container's entrypoint script. When you are done using the environment you can halt the environment using the command: docker compose stop If you'd like to detach volumes (without deleting them), you can also use the command. docker compose down","title":"Installation"},{"location":"install/#fieldsets-pipeline-installation-guide","text":"This section covers details on how to get started with the fieldsets pipeline. TL;DR git clone --recurse-submodules git@github.com:Fieldsets/fieldsets-pipeline.git cd fieldsets-pipeline git submodule foreach git pull origin main cp ./env.example ./.env (make appropriate corrections to file. additional example envs found in [./config/docker](./config/docker/) docker-compose up -d --build","title":"Fieldsets Pipeline Installation Guide"},{"location":"install/#system-requirements","text":"Before we begin, you must have the following installed locally on your machine. Follow the links below for directions on how to get setup on your current environment. docker compose git","title":"System Requirements."},{"location":"install/#mac-os","text":"Using the terminal execute the following steps - Install Xcode - xcode-select --install - Install homebrew - /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\" - Disable homebrew analytics - brew analytics off - Install git - brew install git - Install docker-compose - brew install docker-compose - Run caffeinate to prevent sleep on first run - caffeinate","title":"Mac OS"},{"location":"install/#linux-debian-based","text":"Install git sudo apt install docker Install git sudo apt install git","title":"Linux (Debian based)"},{"location":"install/#windows-install-wsl-v2","text":"Install wsl v2 Install Docker Desktop Set DNS using ./config/docker-daemon.json","title":"Windows Install (wsl v2)"},{"location":"install/#install-steps","text":"Once you have install git and docker-compose, the first step is to clone the fieldsets-pipeline repository and all of it's submodules into your local environment. This pipeline has 2 submodules minimally required to run. The postgres database container and a localized scripting container that helps run whatever scripts are needed by the pipeline project. By default, this pipeline will run using all variables from all integrations that are active in production even if the containers are not running. Before getting started, it may be best to look in the docker config sub-directory and utilize the dotenv, gitmodule & compose override files found there in place of the top level files used to run the pipeline in full. git clone --recurse-submodules https://github.com/fieldsets/fieldsets-pipeline.git Next we need head to our repository directory and make sure we have the latest versions of our submodules. You can update them in with the following commands: cd fieldsets-pipeline git submodule foreach git pull origin main While the enviroment will boot with default options, you should make a copy of the example env file and change any configuration parameters (like postgres version or the path of your private key). This is unnecessary as the project will run with the default values set in the docker-compose.yml file, but the project will have little functionality out side of creating a local postgres environment for you. If you are looking for a fieldsets database instance to run on EC2, you may want to look at Fieldsets's docker postgres repository. The example env contains all the variables used to configure our full pipeline, so every project (including yours!) that is integrated into the Fieldsets Automated Pipeline, has it's environment variables added to the env.example file . Depending on what you are looking to do, it is likely that many of these variables will be empty for your application or not defined. You can omit or change these variables to help with current deployments. Defaults will be filled in using values defined in the docker-compose file for any omitted values. Subsets of fieldsets-pipeline project dotenv files can be found in config/docker/ . A full explantion of environment variables can also be found here . NOTE: Other versions of PostgreSQL have not been tested outside of v12 & v13. The philosophy behind upgrades is to do it after it has established thorough adoption by the community. cp ./env.example ./.env Now lets get the environment running! For our remote data stores to work, we will need credentials for our production and clone servers. Once docker-compose is installed and you have cloned this repository as instructed above, you can get the entire environment up and running with the following command: docker compose up -d That's it! You should be up and running. The first docker build may take a while as it imports data remotely and can vary depending on network speed and your local environment hardware. If you'd like to view what is happening on the install you can track the output logs of the container that does all the heavy lifting with the following command: docker compose logs -f fieldsets-local or simply check everything docker compose logs -f If you see a log entry of a container exited with code 0 that means everything has run succssfully. Anyother exit code besides 0 means something went wrong somewhere along the line. You you have set ENABLE_TERMINAL=true you won't see an exit with code 0 message when a container completes. You should simply see the final message output by the container's entrypoint script. When you are done using the environment you can halt the environment using the command: docker compose stop If you'd like to detach volumes (without deleting them), you can also use the command. docker compose down","title":"Install Steps"},{"location":"pipeline/","text":"Data Architecture & Pipeline For the more information on the data structures and pipeline setup , see the documentation here . This repository defines Fieldsets's data pipeline from and was built following relational theory and applying normal form where relational data exists. We try to stay within 3NF but make exceptions where there is a clear performance benefit for visualization. This repository also defines non-normalized data structures for functions including but not exclusive to data aggregation, metric creation, positional analysis & time-series data. All of these optimized data structures can be found within the database fieldsets under the schema fieldsets . We utilize these optimized data structures to abstractly recreate the current fieldsets schema found in the database fieldsets under the schema public so that our team can experience the performance benefit with little disruption to their work flow.","title":"Data Architecture & Pipeline"},{"location":"pipeline/#data-architecture-pipeline","text":"For the more information on the data structures and pipeline setup , see the documentation here . This repository defines Fieldsets's data pipeline from and was built following relational theory and applying normal form where relational data exists. We try to stay within 3NF but make exceptions where there is a clear performance benefit for visualization. This repository also defines non-normalized data structures for functions including but not exclusive to data aggregation, metric creation, positional analysis & time-series data. All of these optimized data structures can be found within the database fieldsets under the schema fieldsets . We utilize these optimized data structures to abstractly recreate the current fieldsets schema found in the database fieldsets under the schema public so that our team can experience the performance benefit with little disruption to their work flow.","title":"Data Architecture &amp; Pipeline"},{"location":"todo/","text":"Pipe Line Improvements Here we keep a list of things that can be improved or fixed. Integrate Docker Secrets Build logic in container phases for adding in plugin based compose files Future Integrations MinIO instead of S3 Redash Juypter Notebooks R-Studio Portainer Terraform","title":"Pipe Line Improvements"},{"location":"todo/#pipe-line-improvements","text":"Here we keep a list of things that can be improved or fixed. Integrate Docker Secrets Build logic in container phases for adding in plugin based compose files","title":"Pipe Line Improvements"},{"location":"todo/#future-integrations","text":"MinIO instead of S3 Redash Juypter Notebooks R-Studio Portainer Terraform","title":"Future Integrations"},{"location":"tools/","text":"Tools Tool Description Run Cmd Example terminal.sh Connect to container terminal shell. ./bin/terminal.sh CONTAINER_NAME ./bin/terminal.sh fieldsets-local postgres-cli.sh Run psql CLI within docker-postgres container. Connects to env var ${POSTGRES_HOST} ./bin/postgres-cli.sh ./bin/postgres-cli.sh run.sh Run a script (such as an init script) within fieldsets-local. It copies the contents of the current directory of the script to /tmp/ in fieldsets-local and runs it. It deletes it upon completion. ./bin/run.sh ./bin/run.sh SCRIPT_NAME","title":"Tools"},{"location":"tools/#tools","text":"Tool Description Run Cmd Example terminal.sh Connect to container terminal shell. ./bin/terminal.sh CONTAINER_NAME ./bin/terminal.sh fieldsets-local postgres-cli.sh Run psql CLI within docker-postgres container. Connects to env var ${POSTGRES_HOST} ./bin/postgres-cli.sh ./bin/postgres-cli.sh run.sh Run a script (such as an init script) within fieldsets-local. It copies the contents of the current directory of the script to /tmp/ in fieldsets-local and runs it. It deletes it upon completion. ./bin/run.sh ./bin/run.sh SCRIPT_NAME","title":"Tools"},{"location":"developer/","text":"Fieldsets Developer Resources","title":"Fieldsets Developer Resources"},{"location":"developer/#fieldsets-developer-resources","text":"","title":"Fieldsets Developer Resources"},{"location":"developer/coding-standards/","text":"Fieldsets Coding Standards Coding standards are collections of coding rules, guidelines, and best practices. Standards are not hard rules that need to be followed all the time. There are always exceptions, but the main focus for the standards at Fieldsets is to establish readablity and familiarity of a code base that will be interacted with by numerous teams. While Fieldsets has a large number of legacy code bases that do not apply these coding standards, the goal is moving forward to have all new code written following these standards. While not expected, you may find as you work with older codebases that some code would benefit from the improved readablilty of standardized code. Feel free to clean up any legacy code as you see fit. Overall there are a few rules that apply to developing a readable code base: No commented code. If it is not used remove it before it is pushed to production. No extra white space. Use section comment headers to create visual breakpoints. Any comments left in code, should be descriptive about the approach in the following code or why it was written this way. Use a linter to help keep your code clean. Use functions/classes whenever possible for code reuse and readabliity. Document all functions and classes with comment headers. Before merging code to production, ask for a code review.","title":"Fieldsets Coding Standards"},{"location":"developer/coding-standards/#fieldsets-coding-standards","text":"Coding standards are collections of coding rules, guidelines, and best practices. Standards are not hard rules that need to be followed all the time. There are always exceptions, but the main focus for the standards at Fieldsets is to establish readablity and familiarity of a code base that will be interacted with by numerous teams. While Fieldsets has a large number of legacy code bases that do not apply these coding standards, the goal is moving forward to have all new code written following these standards. While not expected, you may find as you work with older codebases that some code would benefit from the improved readablilty of standardized code. Feel free to clean up any legacy code as you see fit. Overall there are a few rules that apply to developing a readable code base: No commented code. If it is not used remove it before it is pushed to production. No extra white space. Use section comment headers to create visual breakpoints. Any comments left in code, should be descriptive about the approach in the following code or why it was written this way. Use a linter to help keep your code clean. Use functions/classes whenever possible for code reuse and readabliity. Document all functions and classes with comment headers. Before merging code to production, ask for a code review.","title":"Fieldsets Coding Standards"},{"location":"developer/coding-standards/R/","text":"Where applicable we follow Tidyverse's R Style Guide where standards are not defined below. Use explicit returns Do not rely on R\u2019s implicit return feature. It is better to be clear about your intent to return() an object. # Good AddValues <- function(x, y) { return(x + y) } # Bad AddValues <- function(x, y) { x + y } Qualifying namespaces Users should explicitly qualify namespaces for all external functions. # Good purrr::map() We discourage using the @import Roxygen tag to bring in all functions into a NAMESPACE. Google has a very big R codebase, and importing all functions creates too much risk for name collisions. While there is a small performance penalty for using ::, it makes it easier to understand dependencies in your code. There are some exceptions to this rule. Infix functions (%name%) always need to be imported. Certain rlang pronouns, notably .data, need to be imported. Functions from default R packages, including datasets, utils, grDevices, graphics, stats and methods. If needed, you can @import the full package. When importing functions, place the @importFrom tag in the Roxygen header above the function where the external dependency is used.","title":"R"},{"location":"developer/coding-standards/R/#use-explicit-returns","text":"Do not rely on R\u2019s implicit return feature. It is better to be clear about your intent to return() an object. # Good AddValues <- function(x, y) { return(x + y) } # Bad AddValues <- function(x, y) { x + y }","title":"Use explicit returns"},{"location":"developer/coding-standards/R/#qualifying-namespaces","text":"Users should explicitly qualify namespaces for all external functions. # Good purrr::map() We discourage using the @import Roxygen tag to bring in all functions into a NAMESPACE. Google has a very big R codebase, and importing all functions creates too much risk for name collisions. While there is a small performance penalty for using ::, it makes it easier to understand dependencies in your code. There are some exceptions to this rule. Infix functions (%name%) always need to be imported. Certain rlang pronouns, notably .data, need to be imported. Functions from default R packages, including datasets, utils, grDevices, graphics, stats and methods. If needed, you can @import the full package. When importing functions, place the @importFrom tag in the Roxygen header above the function where the external dependency is used.","title":"Qualifying namespaces"},{"location":"developer/coding-standards/docker/","text":"Docker Coding Standards https://docs.docker.com/develop/develop-images/dockerfile_best-practices/","title":"Docker Coding Standards"},{"location":"developer/coding-standards/docker/#docker-coding-standards","text":"https://docs.docker.com/develop/develop-images/dockerfile_best-practices/","title":"Docker Coding Standards"},{"location":"developer/coding-standards/python/","text":"Where applicable we follow guidelines defined in Python Enhancement Proposal 8 (PEP 8) which is still the core standards followed with the most recent PEP version. A good summary can be found here: https://www.zenesys.com/blog/python-coding-standards-best-practices","title":"Python"},{"location":"developer/coding-standards/shell/","text":"Shell coding standards Where applicable we follow Google's Shell Style Guide where standards are not defined below. Make sure you have installed a linter in your editor of choice to help keep your code consistent. We recommend shellcheck as a linter that is widely supported in most editors. You can install shellcheck with a simple brew install shellcheck command on MacOS, and then install the linter extension into your editor. Fieldsets Specific Coding Standards Differences Indentation - Use 4 spaces instead of 2, just like common programming languages. Shebang - #!/usr/bin/env bash , not #!/usr/bin/bash Table of Content Shell Files File Extension Shebang Include Files Naming Convention Varible Function Source Filename Formating Indentation Lines Variables Arrays Heredoc Quoting Control Structures If Statement For Statement While Statement Case Statement Comments File Headers Function Headers Section Headers Todo Comments Shell Files File Extension [x] Executables should have a .sh extension. [x] Libraries (included files) should have a .sh extension. 00-init.sh Shebang [x] Shebang lines must always be #!/usr/bin/env bash #!/usr/bin/env searches PATH for bash . This allows you to change your PATH to get the interpreter without having to edit every file you're working on. #!/usr/bin/env bash Include Files [x] 1. The included files should not have shebang lines. [x] 2. Use source instead of . to include files. Although . is POSIX standard, the source word provides more readability. Yes source \"${CODE_PATH}/inc/functions.sh\" No # Use source keyword provides more readability. . \"${CODE_PATH}/inc/functions.sh\" Naming Convention Variable [x] 1. Always use underscore naming convention. [x] 2. No camel-case naming convention. [x] 3. If a variable can be changed from its parent environment, it should be in uppercase. [x] 4. If a variable can be changed from a command line argument, it could be in uppercase, and name it starting with an underscore. [x] 5. Other varibles should be lowercase. Yes fieldsets_variable=\"1\" No # No camel-case naming convention. fieldsetsVariable=\"1\" Function [x] 1. Always use underscore naming convention. [x] 2. No camel-case naming convention. [x] 3. A function name should be lowercase. [x] 4. Braces must be on the same line as the function name. Yes func_test() { local this_is_local=\"Local variable\" echo ${this_is_local} } func_test No # Braces must be on the same line as the function name. func_test() { local this_is_local=\"Local variable\" echo ${this_is_local} } func_test Source Filename [x] Lowercase, with hyphens to separate words if needed. Formating Indentation [x] 1. Indent 4 spaces for each level of indentation. [x] 2. No Tabs [x] 3. Vertical indentation in array is 4 spaces and up to five values per line. ( example ) if [ \"${abc}\" == \"yes\" ]; then echo \"ok\" fi Lines [x] 1. No unnecessary semicolons ; . [x] 2. No trailing spaces in the end of line. [x] 3. No spaces in empty line. Variables [x] 1. Using the local keyword inside functions prevents problems with global variables. [x] 2. Always quote the value, unless the value is Integer. [x] 3. Always brace-quote the variables when using them, except single character shell specials. (ex. ? , * , # , etc..) Yes version=\"1.0\" No 3 # Missing quotes. version=1.0 Yes func_test() { local this_is_local=\"Local variable\" echo ${this_is_local} } func_test No # Missing local keyword when defining a varible in a function. func_test() { this_is_local=\"Local variable\" echo ${this_is_local} } func_test Arrays [x] 1. Values on a single line. Single space between values. No space next to parenthesis. [x] 2. Always quotes the values in the array, unless index. Yes modules=(\"bcmath\" \"bz2\" \"cgi\" \"cli\" \"common\" \"zip\") Yes array=(\"one\" \"two\" \"three\" \"four\" [6]=\"five\") No modules=( bcmath bz2 cgi cli common zip ) Heredoc [x] 1. Delimiting identifier should always be uppercase. [x] 2. No semicolon ; after delimiting identifier. [x] 3. Redirect and the delimiting identifier should be separated by a space. YES cat << EOF This is an example. easybash EOF No # Delimiting identifier should be uppercase. cat << eof This is an example. easybash eof No # Semicolon is unnecessary. cat << EOF; This is an example. easybash EOF No # Missing a sapce between redirect and the delimiting identifier. cat <<EOF This is an example. easybash EOF Quoting [x] Always quote strings containing variables, command substitutions, spaces or shell meta characters, unless careful unquoted expansion is required. [x] Double quotes is strongly preferred. Control Structures If Statement [x] ; and then should be on the same line, separated by a space. [x] else \u3001 elif \u3001 fi should be on its own line vertically aligned with the if statement. status=\"error\" # if..fi statement if [ \"${status}\" == \"success\" ]; then echo \"success\" else echo \"other\" fi # if..elif..fi statement if [ \"${status}\" == \"success\" ]; then echo \"success\" elif [ \"${status}\" == \"error\" ]; then echo \"error\" else echo \"other\" fi For Statement [x] ; and do should be on the same line, separated by a space. [x] done should be on its own line vertically aligned with the for statement for module in ${modules[@]}; do func_msg info \"Proceeding to install module \\\"${module}\\\" ...\" sudo ${_PM} install -y app${package_version}-${module} done While Statement [x] ; and do should be on the same line, separated by a space. [x] done should be on its own line vertically aligned with the while statement. i=0 while [ ${i} -lt 5 ]; do i=$(($i+1)) echo ${i} done Case Statement [x] Indent 4 spaces. [x] The patterns \"action\") and the corresponding action terminator ;; are indented at the same level. [x] The pattern strings should be around with double quotes to keep readability. [x] Do not quote pattern-matching metacharacters. ( * , ? , | , etc...) while [ \"$#\" -gt 0 ]; do case \"$1\" in \"-v\") package_version=\"${2}\" shift 2 ;; \"--version=\"*) package_version=\"${1#*=}\"; shift 1 ;; # Help \"-h\"|\"--help\") show_script_help exit 1 ;; # Info \"-i\"|\"--information\") show_script_information exit 1 ;; \"-\"*) echo \"Unknown option: ${1}\" exit 1 ;; *) echo \"Unknown option: ${1}\" exit 1 ;; esac done Comments File Header [x] Include descriptive file header summarizing purpose of script and any relative information. [x] Prefix any parameters, dependencies, includes or environment variables in header with an @ [x] Utilize #=== to open and close header comment. Yes #=== # 00-init.sh: Description of 00-init.sh # Other useful infomation # # @envvar VERSION | String # @envvar ENVIRONMENT | String # @param arg1 | String # @dependency ./data.csv | File # @include ./lib/functions.sh | File # #=== Function Headers [x] Include descriptive function header summarizing purpose of script and any relative information. [x] Prefix any parameters, dependencies, includes or environment variables in header with an @ [x] Utilize ## to open and close function header. ## # wait_for_threads: Look at pids array and wait for all pids to complete. # @depends pids ## wait_for_threads() { # Our threads are running in the background. Lets wait for all of them to complete. for p in \"${pids[@]}\"; do echo \"Waiting for process id ${p}......\" wait \"${p}\" 2>/dev/null echo \"Thread with PID ${p} has completed\" done pids=(); } Section Headers [x] Use a comment as a section header with the defined section as either Variables , Functions or Main . [x] Utilize #=== to open and close section header. #=== # Variables #=== pids=() #=== # Functions #=== ## # traperr: Better error handling ## traperr() { echo \"ERROR: ${BASH_SOURCE[1]} at about ${BASH_LINENO[0]}\" } ## # wait_for_threads: Look at pids array and wait for all pids to complete. # @depends pids ## wait_for_threads() { # Our threads are running in the background. Lets wait for all of them to complete. for p in \"${pids[@]}\"; do echo \"Waiting for process id ${p}......\" wait \"${p}\" 2>/dev/null echo \"Thread with PID ${p} has completed\" done pids=(); } ## # init_server: setup container config ## init_server() { echo \"Initializing server....\"; } #=== # Main #=== trap traperr ERR init_server","title":"Shell coding standards"},{"location":"developer/coding-standards/shell/#shell-coding-standards","text":"Where applicable we follow Google's Shell Style Guide where standards are not defined below. Make sure you have installed a linter in your editor of choice to help keep your code consistent. We recommend shellcheck as a linter that is widely supported in most editors. You can install shellcheck with a simple brew install shellcheck command on MacOS, and then install the linter extension into your editor.","title":"Shell coding standards"},{"location":"developer/coding-standards/shell/#fieldsets-specific-coding-standards","text":"Differences Indentation - Use 4 spaces instead of 2, just like common programming languages. Shebang - #!/usr/bin/env bash , not #!/usr/bin/bash","title":"Fieldsets Specific Coding Standards"},{"location":"developer/coding-standards/shell/#table-of-content","text":"Shell Files File Extension Shebang Include Files Naming Convention Varible Function Source Filename Formating Indentation Lines Variables Arrays Heredoc Quoting Control Structures If Statement For Statement While Statement Case Statement Comments File Headers Function Headers Section Headers Todo Comments","title":"Table of Content"},{"location":"developer/coding-standards/shell/#shell-files","text":"","title":"Shell Files"},{"location":"developer/coding-standards/shell/#file-extension","text":"[x] Executables should have a .sh extension. [x] Libraries (included files) should have a .sh extension. 00-init.sh","title":"File Extension"},{"location":"developer/coding-standards/shell/#shebang","text":"[x] Shebang lines must always be #!/usr/bin/env bash #!/usr/bin/env searches PATH for bash . This allows you to change your PATH to get the interpreter without having to edit every file you're working on. #!/usr/bin/env bash","title":"Shebang"},{"location":"developer/coding-standards/shell/#include-files","text":"[x] 1. The included files should not have shebang lines. [x] 2. Use source instead of . to include files. Although . is POSIX standard, the source word provides more readability. Yes source \"${CODE_PATH}/inc/functions.sh\" No # Use source keyword provides more readability. . \"${CODE_PATH}/inc/functions.sh\"","title":"Include Files"},{"location":"developer/coding-standards/shell/#naming-convention","text":"","title":"Naming Convention"},{"location":"developer/coding-standards/shell/#variable","text":"[x] 1. Always use underscore naming convention. [x] 2. No camel-case naming convention. [x] 3. If a variable can be changed from its parent environment, it should be in uppercase. [x] 4. If a variable can be changed from a command line argument, it could be in uppercase, and name it starting with an underscore. [x] 5. Other varibles should be lowercase. Yes fieldsets_variable=\"1\" No # No camel-case naming convention. fieldsetsVariable=\"1\"","title":"Variable"},{"location":"developer/coding-standards/shell/#function","text":"[x] 1. Always use underscore naming convention. [x] 2. No camel-case naming convention. [x] 3. A function name should be lowercase. [x] 4. Braces must be on the same line as the function name. Yes func_test() { local this_is_local=\"Local variable\" echo ${this_is_local} } func_test No # Braces must be on the same line as the function name. func_test() { local this_is_local=\"Local variable\" echo ${this_is_local} } func_test","title":"Function"},{"location":"developer/coding-standards/shell/#source-filename","text":"[x] Lowercase, with hyphens to separate words if needed.","title":"Source Filename"},{"location":"developer/coding-standards/shell/#formating","text":"","title":"Formating"},{"location":"developer/coding-standards/shell/#indentation","text":"[x] 1. Indent 4 spaces for each level of indentation. [x] 2. No Tabs [x] 3. Vertical indentation in array is 4 spaces and up to five values per line. ( example ) if [ \"${abc}\" == \"yes\" ]; then echo \"ok\" fi","title":"Indentation"},{"location":"developer/coding-standards/shell/#lines","text":"[x] 1. No unnecessary semicolons ; . [x] 2. No trailing spaces in the end of line. [x] 3. No spaces in empty line.","title":"Lines"},{"location":"developer/coding-standards/shell/#variables","text":"[x] 1. Using the local keyword inside functions prevents problems with global variables. [x] 2. Always quote the value, unless the value is Integer. [x] 3. Always brace-quote the variables when using them, except single character shell specials. (ex. ? , * , # , etc..) Yes version=\"1.0\" No 3 # Missing quotes. version=1.0 Yes func_test() { local this_is_local=\"Local variable\" echo ${this_is_local} } func_test No # Missing local keyword when defining a varible in a function. func_test() { this_is_local=\"Local variable\" echo ${this_is_local} } func_test","title":"Variables"},{"location":"developer/coding-standards/shell/#arrays","text":"[x] 1. Values on a single line. Single space between values. No space next to parenthesis. [x] 2. Always quotes the values in the array, unless index. Yes modules=(\"bcmath\" \"bz2\" \"cgi\" \"cli\" \"common\" \"zip\") Yes array=(\"one\" \"two\" \"three\" \"four\" [6]=\"five\") No modules=( bcmath bz2 cgi cli common zip )","title":"Arrays"},{"location":"developer/coding-standards/shell/#heredoc","text":"[x] 1. Delimiting identifier should always be uppercase. [x] 2. No semicolon ; after delimiting identifier. [x] 3. Redirect and the delimiting identifier should be separated by a space. YES cat << EOF This is an example. easybash EOF No # Delimiting identifier should be uppercase. cat << eof This is an example. easybash eof No # Semicolon is unnecessary. cat << EOF; This is an example. easybash EOF No # Missing a sapce between redirect and the delimiting identifier. cat <<EOF This is an example. easybash EOF","title":"Heredoc"},{"location":"developer/coding-standards/shell/#quoting","text":"[x] Always quote strings containing variables, command substitutions, spaces or shell meta characters, unless careful unquoted expansion is required. [x] Double quotes is strongly preferred.","title":"Quoting"},{"location":"developer/coding-standards/shell/#control-structures","text":"","title":"Control Structures"},{"location":"developer/coding-standards/shell/#if-statement","text":"[x] ; and then should be on the same line, separated by a space. [x] else \u3001 elif \u3001 fi should be on its own line vertically aligned with the if statement. status=\"error\" # if..fi statement if [ \"${status}\" == \"success\" ]; then echo \"success\" else echo \"other\" fi # if..elif..fi statement if [ \"${status}\" == \"success\" ]; then echo \"success\" elif [ \"${status}\" == \"error\" ]; then echo \"error\" else echo \"other\" fi","title":"If Statement"},{"location":"developer/coding-standards/shell/#for-statement","text":"[x] ; and do should be on the same line, separated by a space. [x] done should be on its own line vertically aligned with the for statement for module in ${modules[@]}; do func_msg info \"Proceeding to install module \\\"${module}\\\" ...\" sudo ${_PM} install -y app${package_version}-${module} done","title":"For Statement"},{"location":"developer/coding-standards/shell/#while-statement","text":"[x] ; and do should be on the same line, separated by a space. [x] done should be on its own line vertically aligned with the while statement. i=0 while [ ${i} -lt 5 ]; do i=$(($i+1)) echo ${i} done","title":"While Statement"},{"location":"developer/coding-standards/shell/#case-statement","text":"[x] Indent 4 spaces. [x] The patterns \"action\") and the corresponding action terminator ;; are indented at the same level. [x] The pattern strings should be around with double quotes to keep readability. [x] Do not quote pattern-matching metacharacters. ( * , ? , | , etc...) while [ \"$#\" -gt 0 ]; do case \"$1\" in \"-v\") package_version=\"${2}\" shift 2 ;; \"--version=\"*) package_version=\"${1#*=}\"; shift 1 ;; # Help \"-h\"|\"--help\") show_script_help exit 1 ;; # Info \"-i\"|\"--information\") show_script_information exit 1 ;; \"-\"*) echo \"Unknown option: ${1}\" exit 1 ;; *) echo \"Unknown option: ${1}\" exit 1 ;; esac done","title":"Case Statement"},{"location":"developer/coding-standards/shell/#comments","text":"","title":"Comments"},{"location":"developer/coding-standards/shell/#file-header","text":"[x] Include descriptive file header summarizing purpose of script and any relative information. [x] Prefix any parameters, dependencies, includes or environment variables in header with an @ [x] Utilize #=== to open and close header comment. Yes #=== # 00-init.sh: Description of 00-init.sh # Other useful infomation # # @envvar VERSION | String # @envvar ENVIRONMENT | String # @param arg1 | String # @dependency ./data.csv | File # @include ./lib/functions.sh | File # #===","title":"File Header"},{"location":"developer/coding-standards/shell/#function-headers","text":"[x] Include descriptive function header summarizing purpose of script and any relative information. [x] Prefix any parameters, dependencies, includes or environment variables in header with an @ [x] Utilize ## to open and close function header. ## # wait_for_threads: Look at pids array and wait for all pids to complete. # @depends pids ## wait_for_threads() { # Our threads are running in the background. Lets wait for all of them to complete. for p in \"${pids[@]}\"; do echo \"Waiting for process id ${p}......\" wait \"${p}\" 2>/dev/null echo \"Thread with PID ${p} has completed\" done pids=(); }","title":"Function Headers"},{"location":"developer/coding-standards/shell/#section-headers","text":"[x] Use a comment as a section header with the defined section as either Variables , Functions or Main . [x] Utilize #=== to open and close section header. #=== # Variables #=== pids=() #=== # Functions #=== ## # traperr: Better error handling ## traperr() { echo \"ERROR: ${BASH_SOURCE[1]} at about ${BASH_LINENO[0]}\" } ## # wait_for_threads: Look at pids array and wait for all pids to complete. # @depends pids ## wait_for_threads() { # Our threads are running in the background. Lets wait for all of them to complete. for p in \"${pids[@]}\"; do echo \"Waiting for process id ${p}......\" wait \"${p}\" 2>/dev/null echo \"Thread with PID ${p} has completed\" done pids=(); } ## # init_server: setup container config ## init_server() { echo \"Initializing server....\"; } #=== # Main #=== trap traperr ERR init_server","title":"Section Headers"},{"location":"developer/coding-standards/sql/","text":"Where applicable we follow IBM's Coding Guidelines unless specified differently below. Captialize all reserved words","title":"Sql"},{"location":"schemas/","text":"When defining schemas, the framework expects field and set objects to have the following keys. Set JSON Object: { \"token\": String, \"label\": String, \"parent\": String, #Existing Set Token String (optional) \"metadata\": JSON #(optional) } Field JSON Object { \"token\": String, \"label\": String, \"type\": FieldType, \"parent\": String, #Existing Token of field or fieldset if type is defined as fieldset. (optional) \"values\": [Any], #Used for definition of enum type. If enum is declared and values are not defined here, the enum will be caclulated after schmea definition (optional) \"value\": Any, #Default value (optional) \"store\": StoreType, \"metadata\": JSON #(optional) } FieldSet JSON Object { \"token\": String, \"label\": String, \"parent\": String, #Existing Set Token String (optional) \"metadata\": JSON #(optional) \"fields\": [ # List of Field Objects { \"token\": String, \"label\": String, \"type\": FieldType, \"parent\": String, #Existing Token of field or fieldset if type is defined as fieldset. (optional) \"values\": [Any], #Used for definition of enum type. If enum is declared and values are not defined here, the enum will be caclulated after schmea definition (optional) \"value\": Any, #Default value (optional) \"store\": StoreType, \"metadata\": JSON #(optional) } ] } Enums Using the non-static data types for field_type & store_type ( any , custom ), the following enums strings are predfeined when defining a schema. You can utilize these strings to return an enum of the current DB data type tokens for store_type & field_type . This allows us to avoid cases where we have schemas hardcoding a values list that needs to be updated every time a custom type is added. fieldtype storetype","title":"Index"},{"location":"schemas/#enums","text":"Using the non-static data types for field_type & store_type ( any , custom ), the following enums strings are predfeined when defining a schema. You can utilize these strings to return an enum of the current DB data type tokens for store_type & field_type . This allows us to avoid cases where we have schemas hardcoding a values list that needs to be updated every time a custom type is added. fieldtype storetype","title":"Enums"},{"location":"schemas/crm/","text":"users - CRM system user. Can perform data queries. associations - a group for members, users, notes, tags, tasks, conversations, files and other associations members - association members. Can be owners of files, notes, conversations, tasks and associations. tags - categories or terms that can be assigned to any association, member, user, task, note, conversation or file. tasks - an activity assignment that can have start and end dates and be assigned to a different user from it's owner. files - a file address that can be of any format. notes - a basic text note can be customized by setting a custom note type. conversations - records of user/member conversations. Conversations are of predefined types and formatting.","title":"Index"},{"location":"schemas/wealth-management/","text":"Extends CRM Schema.","title":"Index"}]}